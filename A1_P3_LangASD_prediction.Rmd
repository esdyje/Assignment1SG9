---
title: "Assignment 1 - Language Development in ASD - part 3"
author: "Esther Dyngby"
date: "September, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/OneDrive - Aarhus universitet/AU-Cognitive Science/3rd Semester/Experimental Methods 3/Exercise/Assignments/Assignment 1")

#Load data
dat2=read.csv("LU_test.csv")
dat1=read.csv("demo_test.csv")
dat3=read.csv("token_test.csv")
traindata=read.csv("clean_data.csv")
traindata=traindata[,-1]

#load packages
library(MuMIn);library(lmerTest); library(modelr);library(stringr); library(tidyverse);library(plyr);library(merTools); library(caret);
library(Metrics);library(modelr)

```

Note: loops.
For (i IN FOLDS)
Sekect Data
Test model
Save the performance

## Welcome to the third exciting part of the Language Development in ASD exercise

In this exercise we will delve more in depth with different practices of model comparison and model selection, by first evaluating your models from last time, then learning how to cross-validate models and finally how to systematically compare models.

N.B. There are several datasets for this exercise, so pay attention to which one you are using!

1. The (training) dataset from last time (the awesome one you produced :-) ).
2. The (test) datasets on which you can test the models from last time:
* Demographic and clinical data: https://www.dropbox.com/s/ra99bdvm6fzay3g/demo_test.csv?dl=0
* Utterance Length data: https://www.dropbox.com/s/uxtqqzl18nwxowq/LU_test.csv?dl=0
* Word data: https://www.dropbox.com/s/1ces4hv8kh0stov/token_test.csv?dl=0

### Exercise 1) Testing model performance

How did your models from last time perform? In this exercise you have to compare the results on the training data () and on the test data. Report both of them. Compare them. Discuss why they are different.

- recreate the models you chose last time (just write the code again and apply it to Assignment2TrainData1.csv)
- calculate performance of the model on the training data: root mean square error is a good measure. (Tip: google the functions inst and predict() )
- create the test dataset (apply the code from assignment 1 part 1 to clean up the 3 test datasets)
- test the performance of the models on the test data (Tips: time to reuse "predict()")
- optional: predictions are never certain, can you identify the uncertainty of the predictions? (e.g. google predictinterval())

```{r}
#loading model
bestmodel=lme4::lmer(CHI_MLU~visit+MOT_MLU+verbalIQ+(1+visit|ID),traindata, REML=FALSE)
summary(bestmodel)

#performance of the model on previous data
modelr::rmse(bestmodel, traindata)

#clean test data 

#renaming ID-columns
names(dat1)[names(dat1)=="Child.ID"] <- "ID"
names(dat2)[names(dat2)=="SUBJ"] <- "ID"
names(dat3)[names(dat3)=="SUBJ"] <- "ID"

#renaming Visit-columns
names(dat1)[names(dat1)=="Visit"] <- "visit"
names(dat2)[names(dat2)=="VISIT"] <- "visit"
names(dat3)[names(dat3)=="VISIT"] <- "visit"

#removing everything else than digits from the column "visit".
dat2$visit = str_extract(dat2$visit, "\\d")
dat3$visit = str_extract(dat3$visit, "\\d")

#removing all "." from all ID columns on all datasets
dat1$ID=gsub("\\.","",dat1$ID)
dat2$ID=gsub("\\.","",dat2$ID)
dat3$ID=gsub("\\.","",dat3$ID)

#To rename variables
dat1=plyr::rename(dat1, c("MullenRaw" = "nonVerbalIQ"))
dat1=plyr::rename(dat1, c("ExpressiveLangRaw" = "verbalIQ"))

#To subset interesting variables
dat1=dplyr::select(dat1, ID, visit, Ethnicity, Gender, Age, Diagnosis, ADOS, nonVerbalIQ, verbalIQ)
dat2=dplyr::select(dat2, ID, visit, MOT_MLU, MOT_LUstd, CHI_MLU, CHI_LUstd)
dat3=dplyr::select(dat3, ID, visit, types_MOT, types_CHI, tokens_MOT, tokens_CHI)

# To merge data
Dat1=merge(dat1, dat2)
DATA=merge(Dat1, dat3)

# To only get data for visit 1
Visit1_data=subset(DATA, DATA$visit == 1)

#To only select relevant columns
Visit1_data=dplyr::select(Visit1_data, ID, ADOS, nonVerbalIQ, verbalIQ)

#To omit irrelevant NA columns from dataset 
DATA=DATA[,-7:-9]

#To merge datasets back together
DATA = merge(DATA, Visit1_data, by = "ID")

# To make ID anonymous
DATA$ID=as.factor(DATA$ID)
DATA$ID=as.numeric(DATA$ID)

# To change the gender variable
DATA$Gender=as.factor(DATA$Gender)
DATA$Gender=revalue(DATA$Gender, c("1"="M", "2"="F"))

# To change diagnosis variable
DATA$Diagnosis=revalue(DATA$Diagnosis, c("A"="ASD", "B"="TD"))
write.csv(DATA, "clean_testdata.csv")

#load new test data
testdata=read.csv("clean_testdata.csv")
testdata=testdata[,-1]
testdata$ID=testdata$ID+100

#run model on test data
Metrics::rmse(testdata$CHI_MLU, predict(bestmodel, testdata, allow.new.levels=T))

# run the model on the train dataset
Metrics::rmse(traindata$CHI_MLU, predict(bestmodel, traindata, allow.new.levels = T))


PredInt=predictInterval(merMod = bestmodel, newdata = testdata, 
                        level = 0.95, n.sims = 1000,
                        stat = "median", type="linear.prediction",
                        include.resid.var = TRUE)
PredInt

```

root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed.
[HERE GOES YOUR ANSWER]

### Exercise 2) Model Selection via Cross-validation (N.B: ChildMLU!)

One way to reduce bad surprises when testing a model on new data is to train the model via cross-validation. 

In this exercise you have to use cross-validation to calculate the predictive error of your models and use this predictive error to select the best possible model.

- Create the basic model of ChildMLU as a function of Time and Diagnosis (don't forget the random effects!).
- Make a cross-validated version of the model. (Tips: google the function "createFolds";  loop through each fold, train a model on the other folds and test it on the fold)
- Report the results and comment on them.

- Now try to find the best possible predictive model of ChildMLU, that is, the one that produces the best cross-validated results.

- Bonus Question 1: How would you go comparing the performance of the basic model and the cross-validated model on the testing set?
- Bonus Question 2: What is the effect of changing the number of folds? Can you plot RMSE as a function of number of folds?
- Bonus Question 3: compare the cross-validated predictive error against the actual predictive error on the test data
```{r}
#making ID factorial
traindata$ID=as.factor(traindata$ID)
testdata$ID=as.factor(testdata$ID)

#Removing NA values
testdata=na.omit(testdata)
traindata=na.omit(traindata)

#creating folds
folds=createFolds(unique(traindata$ID),4)

#crosvalidating model 1
trainRMSE=NULL
testRMSE=NULL
n=1 

for (fold in folds) {
   #subset of traindata
  train2=subset(traindata,!(ID %in% fold))
  #create a test
  test2=subset(traindata,(ID%in% fold))
  #to train model 
  model1=lmerTest::lmer(CHI_MLU ~ 1+visit + Diagnosis + (1+visit|ID), train2, REML = FALSE)
  #RMSE for train data
  trainRMSE[n]=Metrics::rmse(train2$CHI_MLU, predict(model1, train2))
  #RMSE for testdata
  pre=predict(model1,test2, allow.new.levels=T)
  testRMSE[n]=Metrics::rmse(test2$CHI_MLU, pre)
  n=n+1
  #saving variables
}

#mean performance of the crossvalidated model1
mean(testRMSE)
mean(trainRMSE)

#Crossvalidating model 2
trainRMSE2=NULL
testRMSE2=NULL
n=1 

#New loop for cross validation of "bestmodel"
for (fold in folds) {
   #subset of traindata
  train2=subset(traindata,!(ID %in% fold))
  #create a test
  test2=subset(traindata,(ID%in% fold))
  #to train model 
  model2=lmerTest::lmer(CHI_MLU~visit+MOT_MLU+verbalIQ+(1+visit|ID),train2, REML=FALSE)
  #RMSE for train data
  trainRMSE2[n]=Metrics::rmse(train2$CHI_MLU, predict(model2, train2))
  #RMSE for testdata
  pre=predict(model2,test2, allow.new.levels=T)
  testRMSE2[n]=Metrics::rmse(test2$CHI_MLU, pre)
  n=n+1
  #saving variables
}
#mean performance of the crossvalidated model2
mean(trainRMSE2)
mean(testRMSE2)

#Crossvalidating model 3
trainRMSE3=NULL
testRMSE3=NULL
n=1 

for (fold in folds) {
   #subset of traindata
  train2=subset(traindata,!(ID %in% fold))
  #create a test
  test2=subset(traindata,(ID%in% fold))
  #to train model 
  model3=lmerTest::lmer(CHI_MLU~1+visit*Diagnosis+MOT_MLU+verbalIQ+(1 + visit|ID),train2,REML = FALSE)
  #RMSE for train data
  trainRMSE3[n]=Metrics::rmse(train2$CHI_MLU, predict(model3, train2))
  #RMSE for testdata
  pre=predict(model3,test2, allow.new.levels=T)
  testRMSE3[n]=Metrics::rmse(test2$CHI_MLU, pre)
  n=n+1
  #saving variables
}
#mean performance of the crossvalidated model3
mean(trainRMSE3)
mean(testRMSE3)

```
### Exercise 3) Assessing the single child

Let's get to business. This new kiddo - Bernie - has entered your clinic. This child has to be assessed according to his group's average and his expected development.

Bernie is one of the six kids in the test dataset, so make sure to extract that child alone for the following analysis.

You want to evaluate:

- how does the child fare in ChildMLU compared to the average TD child at each visit? Define the distance in terms of absolute difference between this Child and the average TD.
(Tip: recreate the equation of the model: Y=Intercept+BetaX1+BetaX2, etc; input the average of the TD group  for each parameter in the model as X1, X2, etc.).

- how does the child fare compared to the model predictions at Visit 6? Is the child below or above expectations? (tip: use the predict() function on Bernie's data only and compare the prediction with the actual performance of the child)
```{r}
#remerging data for Bernie
DATAfBernie=merge(Dat1, dat3)

# To only get data for visit 1
Visit1_datafBernie=subset(DATAfBernie, DATAfBernie$visit == 1)

#To only select relevant columns
Visit1_datafBernie=dplyr::select(Visit1_datafBernie, ID, ADOS, nonVerbalIQ, verbalIQ)

#To omit irrelevant NA columns from dataset 
DATAfBernie=DATAfBernie[,-7:-9]

#To merge datasets back together
DATAfBernie = merge(DATAfBernie, Visit1_datafBernie, by = "ID")

#get Bernie data
Berniedata=DATAfBernie[DATAfBernie$ID=="Bernie",]

# To change the gender variable
Berniedata$Gender=as.factor(Berniedata$Gender)
Berniedata$Gender=revalue(Berniedata$Gender, c("1"="M", "2"="F"))

# To change diagnosis variable
Berniedata$Diagnosis=revalue(Berniedata$Diagnosis, c("A"="ASD", "B"="TD"))

#to make a subset of TD data
TDdata=traindata[traindata$Diagnosis=="TD",]

#model w/ TD data
TDmodel=lmer(CHI_MLU ~ MOT_MLU+ verbalIQ + 1+visit + (1+visit|ID), TDdata, REML = FALSE)
summary(TDmodel)


#Prep

VBIQ=mean(TDdata$verbalIQ)
MMLUV1=mean(TDdata$MOT_MLU[TDdata$visit==1])
MMLUV2=mean(TDdata$MOT_MLU[TDdata$visit==2])
MMLUV3=mean(TDdata$MOT_MLU[TDdata$visit==3])
MMLUV4=mean(TDdata$MOT_MLU[TDdata$visit==4])
MMLUV5=mean(TDdata$MOT_MLU[TDdata$visit==5])
MMLUV6=mean(TDdata$MOT_MLU[TDdata$visit==6])

# To compare Bernie to the average TD
# Equation: -1.69 + 0.285*x (VISIT) + 0.4896*x (MOT_MLU) + 0.0456*x (verbalIQ)
myfunction = function(x, y, z) {
  -1.69 + 0.285*x + 0.4896*y + 0.0456*z
}

# Visit 1
TD_visit1=myfunction(1, MMLUV1, VBIQ)
dif_visit1 = Berniedata$CHI_MLU[Berniedata$visit==1] - TD_visit1

# Visit 2
TD_visit2=myfunction(2, MMLUV2 ,VBIQ)
dif_visit2 = Berniedata$CHI_MLU[Berniedata$visit==2] - TD_visit2

# Visit 3
TD_visit3=myfunction(3, MMLUV3 ,VBIQ)
dif_visit3 = Berniedata$CHI_MLU[Berniedata$visit==3] - TD_visit3

# Visit 4
TD_visit4=myfunction(4, MMLUV4 ,VBIQ)
dif_visit4 = Berniedata$CHI_MLU[Berniedata$visit==4] - TD_visit4

# Visit 5
TD_visit5=myfunction(5, MMLUV5 ,VBIQ)
dif_visit5 = Berniedata$CHI_MLU[Berniedata$visit==5] - TD_visit5

# Visit 6
TD_visit6=myfunction(6, MMLUV6 ,VBIQ)
dif_visit6 = Berniedata$CHI_MLU[Berniedata$visit==6] - TD_visit6

# To create a dataframe 
Visits = c(1,2,3,4,5,6)
Category = c("TD", "TD","TD","TD","TD","TD", "Bernie","Bernie","Bernie","Bernie","Bernie","Bernie")
MLU = c(TD_visit1, TD_visit2, TD_visit3, TD_visit4 ,TD_visit5, TD_visit6, Berniedata$CHI_MLU)

Bernie_comp=data.frame(Visits, Category, MLU)

ggplot(Bernie_comp, aes(x = Visits, y= MLU, colour = Category)) + geom_smooth() + geom_point()+ggtitle("Bernie's development compared to an estimation of average TD childrens performance across visits")

#subsetting TD children at visit 6
TDdata6=TDdata[TDdata$visit==6,]
#modelling TD MLU at visit 6
TDmodel6=lme4::lmer(CHI_MLU ~ MOT_MLU+ verbalIQ + 1+visit + (1+visit|ID), TDdata6, REML = FALSE)

Bernie_6 = Berniedata[6,] # To subset visit 6
Bernie_pred=predict(TDmodel, Bernie_6, allow.new.levels = T) 
Bernie_6$CHI_MLU - Bernie_pred 

```
Bernies development in MLU is in the most case better than TD-children of the traindata. For the first visit he performs better by 0.38 MLU. The second visit he performs worse than the MLU of TD-children, to be exact his MLU is short by -0.18. The third visit he is performing a great deal better with a value of 0.71. The 4th visit he is better than TD children by 0.18, and the 5th by 0.13 and the 6th visit he performs better by 0.22 MLU. All in all, Bernie performs better than TD-children with same verbal intelligence as him. 


### OPTIONAL: Exercise 4) Model Selection via Information Criteria
Another way to reduce the bad surprises when testing a model on new data is to pay close attention to the relative information criteria between the models you are comparing. Let's learn how to do that!

Re-create a selection of possible models explaining ChildMLU (the ones you tested for exercise 2, but now trained on the full dataset and not cross-validated).

Then try to find the best possible predictive model of ChildMLU, that is, the one that produces the lowest information criterion.

- Bonus question for the optional exercise: are information criteria correlated with cross-validated RMSE? That is, if you take AIC for Model 1, Model 2 and Model 3, do they co-vary with their cross-validated RMSE?
```{r}

```

### OPTIONAL: Exercise 5): Using Lasso for model selection
Welcome to the last secret exercise. If you have already solved the previous exercises, and still there's not enough for you, you can expand your expertise by learning about penalizations. Check out this tutorial: http://machinelearningmastery.com/penalized-regression-in-r/ and make sure to google what penalization is, with a focus on L1 and L2-norms. Then try them on your data!


